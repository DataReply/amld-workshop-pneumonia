{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**CNN - Basic convolutional neural networks**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Recommended reads: \n",
    "* https://medium.freecodecamp.org/an-intuitive-guide-to-convolutional-neural-networks-260c2de0a050\n",
    "* https://towardsdatascience.com/applied-deep-learning-part-4-convolutional-neural-networks-584bc134c1e2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Convolutional neural network:**\n",
    "![img 404](https://cdn-images-1.medium.com/max/1600/1*NQQiyYqJJj4PSYAeWvxutg.png)\n",
    "\n",
    "CNN usually consists of a series of convolution + pooling layers, followed by fully connected layers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Convolutions:** <br />\n",
    "Definitions:\n",
    "<img src=\"https://cdn-images-1.medium.com/max/1600/1*cTEp-IvCCUYPTT0QpE3Gjg@2x.png\" alt=\"Drawing\" style=\"width: 400px;\"/>\n",
    "\n",
    "**Input** - input image, usually a 3D array of RGB values. <br />\n",
    "**Kernel** - filter, matrix operation that will be applied to transform input image <br />\n",
    "\n",
    "Convolution - cell 1\n",
    "<img src=\"https://cdn-images-1.medium.com/max/1600/1*ghaknijNGolaA3DpjvDxfQ@2x.png\" alt=\"Drawing\" style=\"width: 400px;\"/>\n",
    "Convolution - cell 2\n",
    "<img src=\"https://cdn-images-1.medium.com/max/1600/1*oxOsZPfZFxgGZw2ycQnenw@2x.png\" alt=\"Drawing\" style=\"width: 400px;\"/>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Pooling:** <br />\n",
    "\n",
    "<img src=\"https://cdn-images-1.medium.com/max/1600/1*ReZNSf_Yr7Q1nqegGirsMQ@2x.png\" alt=\"Drawing\" style=\"width: 400px;\"/>\n",
    "\n",
    "MaxPooling takes the maximum value from the selected cells. Main purpose is to downsample the input but keep the most important information\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Demo showcase - let's start with the usual - sample photo of a cat!:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.preprocessing.image import load_img\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# load an image from file\n",
    "raw_image = load_img('./data/cnn_image/image_cat.jpeg', target_size=(224, 224))\n",
    "plt.imshow(raw_image)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are going to use a CNN that has already been trained for us and is publicly available - VGG16. It's a network consisting of 16 layers <a href=\"https://cdn-images-1.medium.com/max/1600/1*U8uoGoZDs8nwzQE3tOhfkw@2x.png\">(see architecture)</a> and has already been pre-trained for us on <a href=\"http://www.image-net.org/\"> Imagenet dataset </a>.\n",
    "\n",
    "Due to it's popularity VGG16 is already included in some of the most popular ML frameworks - we will use the one  <a href=\"https://keras.io/applications/#vgg16\">provided by Keras</a>.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.applications.vgg16 import VGG16\n",
    "\n",
    "model = VGG16(weights = 'imagenet', input_shape=(224, 224, 3))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can convert our image to array of values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.preprocessing.image import img_to_array\n",
    "from keras.applications.vgg16 import decode_predictions\n",
    "\n",
    "image_arr = img_to_array(raw_image)\n",
    "image_arr.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Due to API requirements, we need an arary of images, therefore we add 1 dimension"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "image = image_arr.reshape((1, image_arr.shape[0], image_arr.shape[1], image_arr.shape[2]))\n",
    "image.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can run the prediction and see the output of our CNN network!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "item = model.predict(image)\n",
    "labels = decode_predictions(item)\n",
    "print(labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Usually it is advised to preprocess the image before feeding it into vgg16. Keras also provides us with functions for that. Let's see how our image would look like after preprocessing:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.applications.vgg16 import preprocess_input\n",
    "\n",
    "image_arr = img_to_array(raw_image)\n",
    "image = image_arr.reshape((1, image_arr.shape[0], image_arr.shape[1], image_arr.shape[2]))\n",
    "\n",
    "# prepare the image for the VGG model\n",
    "image_processed = preprocess_input(image)\n",
    "plt.imshow(image_processed[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's see how it would be classified by a network now!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "item = model.predict(image_processed)\n",
    "labels = decode_predictions(item)\n",
    "print(labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "** Part II - CNN Activation ** <br />\n",
    "Let's try to analize why our network decided to say that the picture is a (persian) cat. We can use <a href=\"https://raghakot.github.io/keras-vis/\">keras-vis</a> module to inspect our vgg16 network.\n",
    "\n",
    "Great example can be found in attached <a href=\"https://github.com/raghakot/keras-vis/blob/master/examples/vggnet/activation_maximization.ipynb\">keras vis notebooks </a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.applications import VGG16\n",
    "from vis.utils import utils\n",
    "from keras import activations\n",
    "\n",
    "# Build the VGG16 network with ImageNet weights\n",
    "model = VGG16(weights='imagenet', include_top=True)\n",
    "\n",
    "# Utility to search for layer index by name.\n",
    "# Alternatively we can specify this as -1 since it corresponds to the last layer.\n",
    "layer_idx = utils.find_layer_idx(model, 'predictions')\n",
    "\n",
    "# Swap softmax with linear\n",
    "model.layers[layer_idx].activation = activations.linear\n",
    "model = utils.apply_modifications(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's have a look how VGG16 \"imagines\" Persian cats to look like. We can expore the activation for \"Persian cat\" class. According to <a href=\"https://gist.github.com/yrevar/942d3a0ac09ec9e5eb3a\">this document</a>, Persian cat's ID is 283. We therefore explore what activate's the neuron responsible for class no. 283. \n",
    "\n",
    "For that, we use <a href=\"https://raghakot.github.io/keras-vis/vis.visualization/#visualize_activation\">visualize_activation</a> from Keras API. This function will iteratively try to generate the input that will maximize the output of the given layer -> in our case we will be checking which input will maximize the output of a neuron that maximizes the \"Persian cat\" class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from vis.visualization import visualize_activation\n",
    "\n",
    "CLASS_ID=283\n",
    "\n",
    "from matplotlib import pyplot as plt\n",
    "%matplotlib inline\n",
    "plt.rcParams['figure.figsize'] = (18, 6)\n",
    "\n",
    "img = visualize_activation(model, layer_idx, filter_indices=CLASS_ID)\n",
    "plt.imshow(img)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's do a couple more iterations on activation function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "img = visualize_activation(model, layer_idx, filter_indices=CLASS_ID, max_iter=500, verbose=True)\n",
    "plt.imshow(img)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Following <a href=\"https://raghakot.github.io/keras-vis/visualizations/activation_maximization/\">the hints</a> of the library's author, let's try to introduce Jitter."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from vis.input_modifiers import Jitter\n",
    "\n",
    "\n",
    "# Jitter 16 pixels along all dimensions to during the optimization process.\n",
    "img = visualize_activation(model, layer_idx, filter_indices=CLASS_ID, max_iter=500, input_modifiers=[Jitter(16)])\n",
    "plt.imshow(img)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the end, let's try to plot heatmap of our CNN and see which part of image triggered it to believe that this is a persian cat. \n",
    "\n",
    "For that, we will use the <a href=\"https://raghakot.github.io/keras-vis/vis.visualization/#visualize_cam\">visualize_cam</a> function from Keras API. \n",
    "\n",
    "Intuition + papre reference for how it works <a href=\"https://jacobgil.github.io/deeplearning/class-activation-maps\">can be found here</a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from vis.visualization import visualize_cam\n",
    "\n",
    "img = visualize_cam(model, layer_idx, filter_indices=CLASS_ID, seed_input=image_processed)\n",
    "plt.subplot(1,2,1)\n",
    "plt.imshow(img)\n",
    "plt.subplot(1,2,2)\n",
    "plt.imshow(image_processed[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Excercise 1.** Try to upload your own picture and see how would it be recognized by the network! You can use <a href=\"https://gist.github.com/yrevar/942d3a0ac09ec9e5eb3a\">Imagenet data</a> to see what classes can you expect!\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Excercise 2.** Pick <a href=\"https://gist.github.com/yrevar/942d3a0ac09ec9e5eb3a\">Imagenet class</a> other than cat. Try to see what activates neuron responsible for it.\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
