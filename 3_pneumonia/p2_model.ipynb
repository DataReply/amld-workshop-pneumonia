{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "d14b5cfa-a0e3-4e14-897e-16b0cc186c0c",
    "_uuid": "7c448bc30163ab2af6f6ffcbd1762915d70ec327"
   },
   "source": [
    "# Part 3.2 Building a model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "688d9d6c-b38e-4121-8bc0-33ffccc757bd",
    "_uuid": "5b08e4c16f36b20700a9334f4a8f7bfb1906105f"
   },
   "outputs": [],
   "source": [
    "from ipynb.fs.defs.p1_data_prep import load_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import shutil\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "import tensorflow as tf "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load datasets for training and validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train, y_train = load_dataset(\"./data/chest_xray/train/\", 6000)\n",
    "x_test, y_test = load_dataset(\"./data/chest_xray/test/\", 6000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(x_train.shape)\n",
    "\n",
    "plt.figure(figsize=(5, 3))\n",
    "\n",
    "y_train_classes = np.argmax(y_train, axis = 1)\n",
    "\n",
    "plt.subplot(1,2,1).set_title('NORMAL')\n",
    "plt.imshow(x_train[np.argmax(y_train_classes == 0)])\n",
    "\n",
    "plt.subplot(1,2,2).set_title('PNEUMONIA')\n",
    "plt.imshow(x_train[np.argmax(y_train_classes == 1)])\n",
    "\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We're going to load VGG16 model with pre-trained ImageNet weights."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](https://www.jeremyjordan.me/content/images/2018/04/vgg16.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we'll replace the orignal classification layer and build a new one:)\n",
    "\n",
    "It will be composed of:\n",
    "\n",
    "- Flatten transformation that reshapes the MaxPool output (4 x 4 x 512) into (1 x 1 x 8192)\n",
    "- Fully Connected Dense layer with Softmax activation function with 2 outputs (1 x 1 x 2)\n",
    "\n",
    "Softmax function normalizes input vector into a probability distribution that sums to 1.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import keras\n",
    "from keras import backend as K\n",
    "from keras.models import Model\n",
    "from keras.layers import Flatten, Dense, BatchNormalization, Dropout\n",
    "from keras.applications.vgg16 import VGG16\n",
    "\n",
    "K.clear_session()\n",
    "\n",
    "NUM_CLASSES = 2\n",
    "\n",
    "base_model = VGG16(weights='imagenet', include_top=False, input_shape=(150, 150, 3))\n",
    "\n",
    "x = base_model.output\n",
    "x = Flatten()(x)\n",
    "x = Dense(NUM_CLASSES, activation='softmax')(x)\n",
    "\n",
    "model = Model(inputs=base_model.input, outputs=x)\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This network has over 14M trainable wegihts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_layers(model):\n",
    "    for idx, layer in enumerate(model.layers):\n",
    "        print(\"layer {}: {}, trainable: {}\".format(idx, layer.name, layer.trainable))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We would like to train only the most bottom classifier layers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for layer in model.layers[0:20]:\n",
    "    layer.trainable = False\n",
    "    \n",
    "print_layers(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.trainable_weights"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "By locking the trainable layers we decrease ammount of trainable parameters to 16'384."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we use the \"cross-entropy\" loss function, which works well for learning probability distributions for classification. \n",
    "\n",
    "See e.g.: https://ml-cheatsheet.readthedocs.io/en/latest/loss_functions.html#cross-entropy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = keras.optimizers.RMSprop()\n",
    "\n",
    "model.compile(loss='categorical_crossentropy',     \n",
    "              optimizer=optimizer, \n",
    "              metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "5c2b5fc4-e1af-4dfe-a928-8a8076c73d59",
    "_uuid": "992129dbd3c7695bdd2e2497a6a56da0227c8c0d"
   },
   "outputs": [],
   "source": [
    "from keras.callbacks import ModelCheckpoint, TensorBoard, ReduceLROnPlateau, EarlyStopping\n",
    "\n",
    "# This callback saves the wieights of the model after each epoch\n",
    "checkpoint = ModelCheckpoint(\n",
    "    'model/weights.epoch_{epoch:02d}.hdf5',\n",
    "    monitor='val_loss', \n",
    "    save_best_only=False, \n",
    "    save_weights_only=False,\n",
    "    mode='auto',\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "# This callback writes logs for TensorBoard\n",
    "tensorboard = TensorBoard(\n",
    "    log_dir='./Graph', \n",
    "    histogram_freq=0,  \n",
    "    write_graph=True\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The training set is un-balanaced. Let's calculate class weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.utils import class_weight\n",
    "y_labels = np.argmax(y_train, axis=1)\n",
    "classweight = class_weight.compute_class_weight('balanced', np.unique(y_labels), y_labels)\n",
    "print(classweight)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# prepare a directory to store the model weights\n",
    "shutil.rmtree('./model')\n",
    "os.makedirs('./model', exist_ok=True)\n",
    "\n",
    "history = model.fit(\n",
    "    x=x_train, y=y_train,\n",
    "    class_weight=classweight,\n",
    "    validation_split=0.3,\n",
    "    callbacks=[checkpoint, tensorboard],\n",
    "    shuffle=True,\n",
    "    batch_size=64,\n",
    "    epochs=20,\n",
    "    verbose=1\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_learning_curves(history):\n",
    "    plt.figure(figsize=(12,4))\n",
    "    \n",
    "    plt.subplot(1,2,1)\n",
    "    plt.plot(history.history['loss'])\n",
    "    plt.plot(history.history['val_loss'])\n",
    "    plt.title('model loss')\n",
    "    plt.ylabel('loss')\n",
    "    plt.xlabel('epoch')\n",
    "    plt.legend(['train', 'val'], loc='upper left')\n",
    "    \n",
    "    plt.subplot(1,2,2)\n",
    "    plt.plot(history.history['acc'])\n",
    "    plt.plot(history.history['val_acc'])\n",
    "    plt.title('model accuracy')\n",
    "    plt.ylabel('accuracy')\n",
    "    plt.xlabel('epoch')\n",
    "    plt.legend(['train', 'val'], loc='upper left')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    \n",
    "plot_learning_curves(history)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Restore the best model** - find an epoch with minimal Loss on the validation dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "idx = np.argmin(history.history['val_loss']) \n",
    "#model.load_weights(\"model/weights.epoch_{:02d}.hdf5\".format(idx + 1))\n",
    "\n",
    "print(\"Loading the best model\")\n",
    "print(\"epoch: {}, val_loss: {}, val_acc: {}\".format(idx + 1, history.history['val_loss'][idx], history.history['val_acc'][idx]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Measure Loss and Accuracy on the test dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "da473dc4-7e79-4be0-97aa-c7cca6e8aa43",
    "_uuid": "1b8d8acad18ea6c063c61c50d84c5c65f8678b21",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "score = model.evaluate(x_test, y_test, verbose=0)\n",
    "print('Model Loss: {}, Accuracy: {}'.format(score[0], score[1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Plot confusion matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "from model_diag import plot_confusion_matrix\n",
    "\n",
    "y_pred = model.predict(x_test)\n",
    "# to get the prediction, we pick the class with with the highest probability\n",
    "y_pred_classes = np.argmax(y_pred, axis = 1) \n",
    "y_true = np.argmax(y_test, axis = 1) \n",
    "\n",
    "conf_mtx = confusion_matrix(y_true, y_pred_classes) \n",
    "plot_confusion_matrix(conf_mtx, target_names = ['NORMAL', 'PNEUMONIA'], normalize=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "... and ROC curve"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import roc_curve\n",
    "from sklearn.metrics import auc\n",
    "\n",
    "fpr = dict()\n",
    "tpr = dict()\n",
    "roc_auc = dict()\n",
    "\n",
    "for i in range(NUM_CLASSES):\n",
    "    fpr[i], tpr[i], _ = roc_curve(y_test[:, i], y_pred[:, i])\n",
    "    roc_auc[i] = auc(fpr[i], tpr[i])\n",
    "                                 \n",
    "plt.figure(figsize=(7, 5))\n",
    "\n",
    "for i in range(NUM_CLASSES):\n",
    "    plt.plot(fpr[i], tpr[i], lw=2,\n",
    "             label='ROC curve of class {0} (area = {1:0.2f})'.format(i, roc_auc[i]))\n",
    "    \n",
    "plt.plot(fpr[0], fpr[0], 'k-', label = 'random guessing')\n",
    "\n",
    "plt.xlabel('False positive rate')\n",
    "plt.ylabel('True positive rate')\n",
    "plt.title('ROC curve')\n",
    "plt.legend(loc=\"lower right\")\n",
    "\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's check the performance metrics when the test set is ballanced (the same number of examples in both classes)\n",
    "\n",
    "We need to under sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from imblearn.under_sampling import RandomUnderSampler\n",
    "from keras.utils.np_utils import to_categorical\n",
    "\n",
    "rus = RandomUnderSampler(random_state=42)\n",
    "\n",
    "x_test_flat_shape = x_test.shape[1] * x_test.shape[2] * x_test.shape[3]\n",
    "x_test_flat = x_test.reshape(x_test.shape[0], x_test_flat_shape)\n",
    "\n",
    "y_test_flat = np.argmax(y_test, axis=1)\n",
    "\n",
    "x_res, y_res = rus.fit_resample(x_test_flat, y_test_flat)\n",
    "\n",
    "print(x_res.shape)\n",
    "print(y_res.shape)\n",
    "\n",
    "y_test_rus = to_categorical(y_res, num_classes = 2)\n",
    "\n",
    "for i in range(len(x_res)):\n",
    "    height, width, channels = 150, 150, 3\n",
    "    x_test_rus = x_res.reshape(len(x_res), height, width, channels)\n",
    "    \n",
    "print(x_test_rus.shape)\n",
    "print(y_test_rus.shape)\n",
    "\n",
    "sns.countplot(np.argmax(y_test_rus, axis=1)).set_title('TEST (undersampled)')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "score = model.evaluate(x_test_rus, y_test_rus, verbose=0)\n",
    "print('Model Loss: {}, Accuracy: {}'.format(score[0], score[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "from model_diag import plot_confusion_matrix\n",
    "\n",
    "y_pred = model.predict(x_test_rus)\n",
    "y_pred_classes = np.argmax(y_pred, axis = 1) \n",
    "y_true = np.argmax(y_test_rus, axis = 1) \n",
    "\n",
    "conf_mtx = confusion_matrix(y_true, y_pred_classes) \n",
    "plot_confusion_matrix(conf_mtx, target_names = ['NORMAL', 'PNEUMONIA'], normalize=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In 50:50 case the accuracy is lower"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Excercise 3.1.** We would like to achive a better performance on the test dataset. Try tuning hyperparameters i.e. learning rate. Can you reach better accuracy?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Excercise 3.2.** Try to use another base network i.e. InceptionV3, ResNet50, DenseNet.\n",
    "\n",
    "What do you observe? Is training time different? How many parameters does the function have?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Excercise 3.3.** Try augmenting the training data using ImageDataGenerator from Keras. Introduce random rotation and image flips. Does this help?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Excercise 3.4.** Try training some of the conv layers. Does this help?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Excercise 3.5.** can you think about a better metric than accuracy, which captures the fact that false negatives are much (much) worse than false positives?"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
