{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Gradient descent example\n",
    "On a linear 1-dimensional function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are a few ways to find linear approximation of the dataset - it can be even done analytically.\n",
    "\n",
    "In this example we present the popular and simple method - <a href=\"https://en.wikipedia.org/wiki/Stochastic_gradient_descent\">Stochastic Gradient Descent (SGD)</a>. This is an iterative approach, in which given a set of input data we try to draw a straight line that with every iteration will have smaller error (error/loss ~~ metric representing summed distance of every point on the plot from that line).\n",
    "\n",
    "We can define a loss function as $L(\\theta)=\\sum_{i^{(i)}\\in data} (f(x^{(i)}) - y^{(i)})^2$, where <br />\n",
    "$y^{(i)}$ - value y corresponding to point x <br />\n",
    "$f(x^{(i)})$ - value of approximating function in point $x^{(i)}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import random\n",
    "plt.rcParams.update({'font.size': 16})\n",
    "from matplotlib import rc\n",
    "rc('text', usetex=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "data_df = pd.read_csv('data.csv')\n",
    "data_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_df.plot.scatter(1, 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_df = pd.read_csv('data.csv')\n",
    "xs = data_df.values[:, 1]\n",
    "ys = data_df.values[:, 2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# helper function to plot loss\n",
    "def plot_fun(t0, t1, i=0, loss=0):\n",
    "    data_df.plot.scatter(1, 2, s=40)\n",
    "    x = 14\n",
    "    plt.plot([0, x], [t0, f(x)], 'k-', lw=2, label=r'$f($chocolate$)$')\n",
    "    plt.axis([0, 13, 0, 35])\n",
    "    plt.legend()\n",
    "    plt.title('iteration = %02d, loss = %.2f' % (i, loss))\n",
    "    plt.tight_layout()\n",
    "    plt.savefig('plots/plot_%03d.png' % (i), dpi=300)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Excercise 1.** Define linear function f(x) that will  be approximating our dataset and depends on two unknown parameters that we will want to find out. - $\\theta_0$ and $\\theta_1$:\n",
    "\n",
    "$f(x)=\\theta_0 + \\theta_1x$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t0 = 0.  # first parameter (theta_0)\n",
    "t1 = 0.  # second parameter (theta_1)\n",
    "\n",
    "def f(x):\n",
    "    # fill here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Excercise 2.** Knowing that \n",
    "\n",
    "$\\frac{\\partial L}{\\partial \\theta_0} = 2\\sum_i (f(x^{(i)}) - y^{(i)})$\n",
    "<br /> and <br />\n",
    "$\\frac{\\partial L}{\\partial \\theta_1} = 2\\sum_i (f(x^{(i)}) - y^{(i)})x_{1}^{(i)}$\n",
    "<br /> Implement the functions to compute those derivatives"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def d_t0(xs, ys):\n",
    "    # fill here\n",
    "\n",
    "    \n",
    "def d_t1(xs, ys):\n",
    "    # fill here\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Excercise 3.** Implement loss function <br />\n",
    "$L(\\theta)=\\sum_{i^{(i)}\\in data} (f(x^{(i)}) - y^{(i)})^2$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fun_loss(xs, ys):\n",
    "    # fill here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "epsilon = 0.0001\n",
    "t0 = 0.\n",
    "t1 = 0.\n",
    "losses = []\n",
    "for i in range(100):\n",
    "    d_t0 = fun_d_t0(xs, ys)\n",
    "    d_t1 = fun_d_t1(xs, ys)\n",
    "    \n",
    "    t0 -= epsilon * d_t0\n",
    "    t1 -= epsilon * d_t1\n",
    "    \n",
    "    loss = fun_loss(xs, ys)\n",
    "    losses.append(loss)\n",
    "    plot_fun(t0, t1, i, loss)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(losses, lw=2)\n",
    "plt.xlabel('Iteration')\n",
    "plt.ylabel('Loss')\n",
    "plt.tight_layout()\n",
    "plt.savefig('plots/loss_over_time.png', dpi=300)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## SPOILER alert - here you can find all the excercises implemented properly\n",
    "\n",
    "t0 = 0. \n",
    "t1 = 0. \n",
    "\n",
    "def f(x):\n",
    "    return t0 + t1 * x\n",
    "\n",
    "def fun_d_t0(xs, ys):\n",
    "    return 2 * sum(np.array([f(x) for x in xs]) - ys)\n",
    "    \n",
    "def fun_d_t1(xs, ys):\n",
    "    return 2 * sum((np.array([f(x) for x in xs]) - ys) * xs)\n",
    "\n",
    "def fun_loss(xs, ys):\n",
    "    return sum((np.array([f(x) for x in xs]) - ys) ** 2)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
